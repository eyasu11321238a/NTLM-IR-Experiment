{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba0f15f-8e4d-4215-b3f2-6cb111dd326a",
   "metadata": {},
   "source": [
    "## <code>NTLM IR Experiment</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e345b44",
   "metadata": {},
   "source": [
    "### Log-Likelihood for Document Ranking and Neural Translation Model for Query Likelihood\n",
    "\n",
    "1. **Log-Likelihood for Document Ranking**:\n",
    "   - Estimates the probability of generating the query $( q )$ given a document $( d )$.\n",
    "   \n",
    "     \n",
    "      $$log p(q|d) = \\sum_{i:c(q_i;d)>0} \\log \\left( \\frac{p(q_i|d)}{\\alpha_d * p(q_i|C)} \\right) + n \\log \\alpha_d \\text{........................Eq(1) }$$\n",
    "      \n",
    "   **Where:**\n",
    "   - $p(q_i|d)$ is the probability of term \\( q_i \\) given document \\( d \\).\n",
    "   - $alpha_d$ is a normalizing factor.\n",
    "   - $p(q_i|C)$ is the probability of term \\( q_i \\) in the collection \\( C \\).\n",
    "2. **Cosine Similarity**\n",
    "    - $$ {cosine\\_similarity}(u, w) = \\frac{u \\cdot w}{|u| |w|} $$\n",
    " \n",
    "     $$p_t(w|u) = \\frac{ \\text{cos}(u', w)}{\\sum_{u_0 \\in V} \\text{cos}(u, w)}   \\text{............................................................................Eq(2) insert it into Eq(3) }$$  \n",
    "\n",
    "\n",
    "3. **Translation Model for Query Likelihood**:\n",
    "   - Models the process of query generation as a translation from document terms to query terms.\n",
    "   \n",
    "     \n",
    "     $$p_t(w|d) = \\sum_{u \\in d} p_t(w|u) p(u|d)  \\text{............................................................Eq(3) }$$ \n",
    "     \n",
    "   **Where:**\n",
    "   - $p_t(w|u)$ is the probability of translating document term $( u )$ into query term $( w )$.\n",
    "   - $p(u|d)$ is the probability of term $( u )$ occurring in document $( d )$.\n",
    "\n",
    "4. **The probability of term $u$ occurring in document $d$**\n",
    "   - $$p(u|d) = \\frac{{\\text{tf}(u, d)}}{{\\sum_{v \\in d} \\text{tf}(v, d)}} \\text{............................................................Eq(4) }$$\n",
    "\n",
    "   **Where:**\n",
    "   - $p(u|d)$ represents the probability of term $( u )$ occurring in document $( d )$.\n",
    "   - $\\text{tf}(u, d)$ denotes the frequency of term $( u )$ in document $( d )$.\n",
    "   - $\\sum_{v \\in d} \\text{tf}(v, d)$ calculates the total number of terms in document $( d )$.\n",
    "   \n",
    "   \n",
    "   \n",
    "5. **Probability of Query Term $( q_i )$ in the Collection $( C )$ ---> $p(q_i|C)$**\n",
    "\n",
    "   The probability  $p(q_i|C)$ represents the likelihood of term $(q_i)$ appearing in the entire collection of documents $(C)$. \n",
    "   It is calculated as the term frequency of $(q_i)$ in the collection $(C)$ divided by the total number of terms in the collection.\n",
    "\n",
    "   - $$p(q_i|C) = \\frac{\\text{cf}(q_i, C)}{\\sum_{v \\in C} \\text{cf}(v, C)} \\text{............................................................Eq(5) }$$\n",
    "\n",
    "   **Where:**\n",
    "   - $\\text{cf}(q_i, C)$ : Collection frequency of term $( q_i )$ in the collection $( C )$.\n",
    "   - $\\sum_{v \\in C} \\text{cf}(v, C)$: Total number of terms in the entire collection ( C )$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. **Connecting the Two Concepts**:\n",
    "   \n",
    "   - Specifically, $ p(q_i|d) $ can be estimated using $ p_t(w|d) $:\n",
    "   \n",
    "     $p_t(w|d) \\approx p_t(q_i|d)$\n",
    "     \n",
    "     $p(u|d)$\n",
    "     \n",
    "     $p(q_i|C)$\n",
    "     \n",
    "   - Substitute these equations in $Eq (1)$\n",
    "\n",
    "7. **Final Log-Likelihood for Document Ranking Equation Eq(1)**:\n",
    "   - Using the Neural translation model, the log-likelihood equation becomes:\n",
    "     \n",
    "     $$log p(q|d) = \\sum_{i:c(q_i;d)>0} \\log \\left( \\frac{\\sum_{u \\in d} p_t(q_i|u) p(u|d)}{\\alpha_d * p(q_i|C)} \\right) + n \\log \\alpha_d$$\n",
    "     \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e0c17a-fdc0-4efe-aa7b-f508fea422db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:33:33.436638Z",
     "start_time": "2024-06-10T09:33:33.431372Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import scipy\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pyterrier as pt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4583629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:33:34.336654Z",
     "start_time": "2024-06-10T09:33:34.330001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME set to: C:\\Program Files\\Java\\jdk-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set JAVA_HOME environment variable\n",
    "java_home = r\"C:\\Program Files\\Java\\jdk-22\"   # adjust your java JDK folder or use relative path in below\n",
    "os.environ[\"JAVA_HOME\"] = java_home\n",
    "\n",
    "# Verify that JAVA_HOME is set correctly\n",
    "print(\"JAVA_HOME set to:\", os.environ.get(\"JAVA_HOME\"))\n",
    "\n",
    "if not pt.started():\n",
    "  pt.init()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d1b451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:33:34.979599Z",
     "start_time": "2024-06-10T09:33:34.885890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 242918\n",
      "Number of terms: 301375\n",
      "Number of postings: 44556521\n",
      "Number of fields: 0\n",
      "Number of tokens: 69541941\n",
      "Field names: []\n",
      "Positions:   false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the relative paths based on the notebook's location\n",
    "DISK45_PATH = os.path.join(\"..\", \"Data\", \"AP_Doc\", \"ap\", \"concatenated\")\n",
    "INDEX_DIR = os.path.join(\"..\", \"Data\", \"AP_Doc\", \"ap\", \"index1\")\n",
    "\n",
    "# Check if the index exists\n",
    "if os.path.exists(os.path.join(INDEX_DIR, \"data.properties\")):\n",
    "    indexref = pt.IndexRef.of(os.path.join(INDEX_DIR, \"data.properties\"))\n",
    "else:    \n",
    "    # Find files in the directory\n",
    "    files = pt.io.find_files(DISK45_PATH)\n",
    "    \n",
    "    # Remove unwanted files\n",
    "    bad = ['/CR/', '/AUX/', 'READCHG', 'READMEFB', 'READFRCG', 'READMEFR', 'READMEFT', 'READMELA']\n",
    "    for b in bad:\n",
    "        files = list(filter(lambda f: b not in f, files))\n",
    "    \n",
    "    # Check if files list is empty and raise an error if it is\n",
    "    if not files:\n",
    "        raise ValueError(f\"No files found in the directory {DISK45_PATH}\")\n",
    "    \n",
    "    # Index the remaining files\n",
    "    indexer = pt.TRECCollectionIndexer(INDEX_DIR, verbose=True)\n",
    "    indexref = indexer.index(files)\n",
    "\n",
    "# Create an index object\n",
    "index = pt.IndexFactory.of(indexref)\n",
    "\n",
    "# collection statistics\n",
    "print(index.getCollectionStatistics().toString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37967072-86dd-4463-b363-d4927841aa6f",
   "metadata": {},
   "source": [
    "## Loading the AP Topics & qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad82fa7-400c-4ee7-bc67-ea7338055854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:33:36.603910Z",
     "start_time": "2024-06-10T09:33:36.529773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the relative paths based on the notebook's location\n",
    "topics_path = os.path.join(\"..\", \"Data\", \"AP_Doc\", \"ap\", \"topics\", \"all_topics_fixed.txt\")\n",
    "qrels_path = os.path.join(\"..\", \"Data\", \"AP_Doc\", \"ap\", \"qrels\", \"AP_only.txt\")\n",
    "index_path = os.path.join(\"..\", \"Data\", \"AP_Doc\", \"ap\", \"index1\")\n",
    "\n",
    "#topics_path = os.path.join(\"..\", \"Data\", \"WSJ_Doc\", \"wsj\", \"topics\", \"all_topics.txt\")\n",
    "#qrels_path = os.path.join(\"..\", \"Data\", \"WSJ_Doc\", \"wsj\", \"qrels\", \"WSJ_only.txt\")\n",
    "#index_path = os.path.join(\"..\", \"Data\", \"WSJ_Doc\", \"wsj\", \"index_WSJ\")\n",
    "\n",
    "# Load topics and qrels from text files\n",
    "topics = pt.io.read_topics(topics_path)\n",
    "qrels = pt.io.read_qrels(qrels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c158cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:33:37.429338Z",
     "start_time": "2024-06-10T09:33:37.407558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>AP880301-0271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>AP880302-0275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>AP880311-0301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>AP880316-0292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>AP880318-0287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63203</th>\n",
       "      <td>200</td>\n",
       "      <td>AP891129-0238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63204</th>\n",
       "      <td>200</td>\n",
       "      <td>AP891130-0262</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63205</th>\n",
       "      <td>200</td>\n",
       "      <td>AP891204-0017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63206</th>\n",
       "      <td>200</td>\n",
       "      <td>AP891214-0236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63207</th>\n",
       "      <td>200</td>\n",
       "      <td>AP891228-0170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63208 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid          docno  label\n",
       "0       51  AP880301-0271      1\n",
       "1       51  AP880302-0275      1\n",
       "2       51  AP880311-0301      1\n",
       "3       51  AP880316-0292      1\n",
       "4       51  AP880318-0287      1\n",
       "...    ...            ...    ...\n",
       "63203  200  AP891129-0238      0\n",
       "63204  200  AP891130-0262      0\n",
       "63205  200  AP891204-0017      0\n",
       "63206  200  AP891214-0236      0\n",
       "63207  200  AP891228-0170      1\n",
       "\n",
       "[63208 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85fc6956f5667d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:33:37.641983Z",
     "start_time": "2024-06-10T09:33:37.638755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>airbus subsidies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>south african sanctions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>leveraged buyouts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>satellite launch contracts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>insider trading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>196</td>\n",
       "      <td>school choice voucher system and its effects u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>197</td>\n",
       "      <td>reform of the jurisprudence system to stop jur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>198</td>\n",
       "      <td>gene therapy and its benefits to humankind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>199</td>\n",
       "      <td>legality of medically assisted suicides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>200</td>\n",
       "      <td>impact of foreign textile imports on u s texti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     qid                                              query\n",
       "0     51                                   airbus subsidies\n",
       "1     52                            south african sanctions\n",
       "2     53                                  leveraged buyouts\n",
       "3     54                         satellite launch contracts\n",
       "4     55                                    insider trading\n",
       "..   ...                                                ...\n",
       "145  196  school choice voucher system and its effects u...\n",
       "146  197  reform of the jurisprudence system to stop jur...\n",
       "147  198         gene therapy and its benefits to humankind\n",
       "148  199            legality of medically assisted suicides\n",
       "149  200  impact of foreign textile imports on u s texti...\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed9d4c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:33:46.295242Z",
     "start_time": "2024-06-10T09:33:38.366637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to parse the TREC file\n",
    "def parse_trec_file(trec_file_path):\n",
    "    doc_texts = {}\n",
    "    current_doc_id = None\n",
    "    current_text = []\n",
    "    \n",
    "    encodings = ['utf-8', 'latin-1', 'ISO-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(trec_file_path, 'r', encoding=encoding, errors='ignore') as file:\n",
    "                for line in file:\n",
    "                    if line.startswith('<DOCNO>'):\n",
    "                        current_doc_id = line.strip().replace('<DOCNO>', '').replace('</DOCNO>', '').strip()\n",
    "                    elif line.startswith('</TEXT>'):\n",
    "                        if current_doc_id:\n",
    "                            doc_texts[current_doc_id] = ' '.join(current_text)\n",
    "                            current_doc_id = None\n",
    "                            current_text = []\n",
    "                    elif current_doc_id:\n",
    "                        if not (line.startswith('<DOC>') or line.startswith('</DOC>') or line.startswith('<FILEID>') or\n",
    "                                line.startswith('<FIRST>') or line.startswith('<SECOND>') or line.startswith('<HEAD>') or\n",
    "                                line.startswith('</BYLINE>') or\n",
    "                                line.startswith('<DATELINE>') or line.startswith('<TEXT>')):\n",
    "                            current_text.append(line.strip())\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue  \n",
    "\n",
    "    return doc_texts\n",
    "\n",
    "# Path to your concatenated TREC file\n",
    "trec_file_path = os.path.join(\"..\", \"Data\", \"AP_Doc\", \"ap\", \"concatenated\", \"concatenated_documents.txt\")\n",
    "#trec_file_path = os.path.join(\"..\", \"Data\", \"WSJ_DOC\", \"wsj\", \"concatenated_WSJ\", \"concatenated_WSJ.txt\")\n",
    "\n",
    "# Parse the document texts\n",
    "doc_texts = parse_trec_file(trec_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe78de8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m doc_texts1 \u001b[38;5;241m=\u001b[39m doc_texts\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdoc_texts1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "doc_texts1 = doc_texts.values()\n",
    "doc_texts1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4b0d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AP880212-0044': \"<BYLINE>By HENRY GOTTLIEB</BYLINE> <BYLINE>Associated Press Writer</BYLINE> Three Eastern European countries have begun talks with the Boeing and McDonnell Douglas aircraft companies to supply long-range passenger planes to replace aging Soviet equipment in their national airlines, Deputy Secretary of State John C. Whitehead said Friday. Whitehead, just back from a two-week trip to four Soviet-bloc countries, told reporters ``my impression is they are very much interested in buying U.S. planes. It's a significant thing, psychologically, I believe _ Eastern European countries flying U.S. planes.'' He declined to identify the countries that expressed interest, but another official, speaking on condition of anonymity, said they were Poland, Hungary and Romania. If deals are arranged it would be the first U.S. aircraft sales to Hungary and Poland. In the early 1970s, Romania purchased several Boeing 707 passenger jets under a contract arranged at a time when the United States was wooing Romania in hopes of increasing that country's independence from Moscow. Whitehead said that from the State Department's viewpoint, ``it would be feasible and desirable'' for the sales, but he noted two major stumbling blocks. Buying U.S. aircraft would require large expenditures by the governments and credits from the U.S. Export-Import Bank for the purchases would not be available, Whitehead said. The aircraft companies, moreover, would be prohibited from selling equipment on a list of military-applicable goods banned for export to communist countries. This could limit the type of airplanes the countries could buy. ``There is some navigational equipment that could not be on the planes,'' Whitehead said. ``They would have to buy stuff that would not be on the (embargo) list.'' Whitehead, the No. 2 official in the State Department, has assumed special responsibility for a U.S. effort to gain influence in countries that have been dominated by the Soviet Union since World War II. Under the policy, the United States has offered the nations increased trade and better relations in general, in return for improved human rights performance and the institution of free economic policies. On his latest trip, Whitehead visited Poland, Romania, Bulgaria and Czechoslovakia. Whitehead said he told Romanian leader Nicolae Ceausescu that his country is in danger of losing special trading rights granted by Congress each year to reward Romania for its relatively free emigration policies. The trading rights _ most-favored-nation status _ have been worth hundreds of millions of dollars to Romania in previous years, but renewal for 1988 has been suspended by Congress because the Ceausescu government's human rights policies are considered repressive, especially toward religious minorities. Whitehead said he told the Romanian leader there was only a ``50-50 chance it would be renewed, but there was still time.'' Ceausescu, in keeping with previous discussions with U.S. officials, refused to discuss the human rights issues on grounds they were strictly an internal matter, Whitehead said. ``I came home with not much reason to hope there will be improvement,'' he said.\",\n",
       " 'AP880212-0045': \"<NOTE>EDS: ADDS 3 grafs Shamir letter to Shultz</NOTE> <BYLINE>By NICOLAS B. TATRO</BYLINE> <BYLINE>Associated Press Writer</BYLINE> Prime Minister Yitzhak Shamir said Friday his coalition partner and rival, Foreign Minister Shimon Peres, was going behind his back in an effort to sabotage Shamir's strategy for Middle East peace. His accusation came the day after Peres suggested that intransigence by Shamir thwarted talks with King Hussein of Jordan, which the foreign minister said could have averted Palestinian riots in the occupied West Bank and Gaza Strip. At the center of the argument over peace is an initiative by the United States. Israeli officials said Secretary of State George P. Shultz telephoned Peres and Shamir on Thursday to say he will visit the Middle East after his scheduled Feb. 21-23 trip to Moscow. U.S. envoy Richard Murphy outlined the new American ideas to Israel's coalition government this week. According to Israeli officials, the plan calls for an international conference, possibly in Geneva, as a means of beginning Arab-Israeli talks by April. Negotiations would focus first on a temporary autonomy plan for the 1.5 million Palestinians in the occupied territories. Talks on the final status of the West Bank and Gaza Strip would begin by December even if self-government had not begun. Peres immediately endorsed the proposals, but Shamir said clarifications were needed, and expressed reservations about the rapid timetable and international role. Shamir leads the right-wing Likud bloc and Peres head the center-left Labor Party. On Israel radio Thursday, the prime minister expressed frustration over what called leaks intended to undercut his negotiating position. ``Every expression of cooperation on my part is thrown in my face immediately,'' he said. ``I show Mr. Peres a letter and after a few hours it is in the hands of all the reporters. ``Everything he does is in the middle of the night. What I do he wants to know about so that he can sabotage it.'' Shamir accused Peres of surrendering to Arab demands for the return of land captured in the 1967 Middle East war. ``How can you conduct negotiations together when your partner runs every day and every minute to the other side and says, `Don't listen to what Shamir says, I'll sell it to you cheaper?''' Shamir declared. Moshe Shahal of the Labor Party rejected Shamir's claim that Peres was ``overeager'' to accept the U.S. proposals. ``We are not being overeager. The prime minister is trying to give an excuse for a period of no movement,'' he said. In a speech to party members Thursday night, Peres indirectly accused Shamir of blocking an international conference agreed to by Jordan. ``If the negotiations with an international opening had begun in April, when they could have been started, would or would it not have spared us the events in the territories?'' Peres said. Riots began Dec. 8 and more than 50 Arabs have been killed, nearly all of them by Israelis, accurding to U.N. figures. In an interview with Hearst Newspapers correspondent John P. Wallach, Shamir said: ``It's not a question of intransigence, of a refusal to talk about it, but I don't want to make the American government fail. ``If we come today with King Hussein and the other Arabs, and try to negotiate about the final status, the negotiations will explode immediately. Israel will say, `We want Israeli sovereignty.' The other side will say, `We want Arab sovereignty' and it is finished. ... It will be over before it starts.'' Shamir said a five-year period was needed between Palestinian autonomy and talks about a final settlement so confidence and trust could be built. He said that was the concept in the 1978 Camp David accords, which led to the peace treaty with Egypt in March 1979. Israel television said Shamir sent a letter to Shultz on Wednesday requesting changes in the U.S. proposal, but Shultz refused. It said Shamir asked the Americans to drop the international conference, extend the autonomy timetable and not to set a date for talks about the final status of the occupied territories. The prime minister also asked the United States to enlist the support of other countries for building urban neighborhoods to house the Gaza Strip's Palestinian refugees, repoort said.\",\n",
       " 'AP880212-0046': 'The Fish and Wildlife Service added 59 species to the list of endangered and threatened species last year, the second highest total ever, the service said Thursday. In the previous most active year, 1979, 68 species were added. The list now stands at 993 animals and plants, of which 513 are found only in foreign countries, the service said in a year-end report. One species was removed from the list after confirmation of its long-suspected extinction _ the Amistad gambusia, a small fish found in some springs in Texas. Another species died out during the year, but the paperwork to remove it from the list has not been completed, spokesman David Klinger said. This was the dusky seaside sparrow, whose last survivor, an elderly male in captivity, died in Florida. Among other reasons to call it a good year, the service noted its reestablishment of the red wolf in the wild in North Carolina, the growth of the Chesapeake Bay bald eagle population from 136 pairs to 161, successful captive breeding of the black-footed ferret and establishment of a second population of California sea otters off the coast near Los Angeles. Too late to make the report was the successful mating of condors in captivity in California. The service removed the last condor from the wild last year on Easter Sunday, and hopes to rebuild the population through breeding in captivity. Environmentalists have complained that under the Reagan administration, the service has been moving far too slowly in making decisions, and that it will take 16 years or so to work through the backlog of about 1,000 candidate species. They have said that in the last 20 years, 300 species became extinct while waiting for a decision. Listing of a species as threatened or endangered under the 1973 Endangered Species Act means federal agencies must consult the service about any of their activities which could harm it. Also, the service must draft and try to carry out a recovery plan, of which 222 are in effect.',\n",
       " 'AP880212-0047': \"<NOTE>Eds: New throughout with comment from Air Force that problem in sensors, not in computers. No pick up.</NOTE> A research satellite launched last week to test elements of the proposed ``Star Wars'' anti-missile shield failed in a secondary tracking excercise when an optical sensor gave false data to two on-board computers, an Air Force official said Friday. Air Force Col. John Otten, assistant director of the Strategic Defense Initiative's kinetic energy office, said an optical sensor on Delta 181, a satellite used to test ``Star Wars'' sensors and trackers, gave flawed data when it tried to track target objects that were beyond its range. Otten said the sensor data went into the computers, causing them to respond inappropriately. He said the flaw was detected within an hour and the computers were told to ignore the data. This corrected the problem. Some of the test data on the system was lost because of the problem, but Otten said the loss was minor because the tracking exercise was a secondary objective. ``In the fundamental mission we suceeded,'' he said. Delta 181 was launched Feb. 8 from the Kennedy Space Center and spent 12 hours conducting a series of tests to gather data needed to refine the SDI, or ``Star Wars,'' anti-missile system. Program manager Andy Green last week called the flight ``a very successful mission.'' However, Aviation Week and Space Technology, in a story prepared for Monday publication, said the satellite was unable to complete ``battle management fire control computations.'' The magazine blamed the problem on the computers, but Otten said the flaw actually was caused by the optical sensor attempting to lock onto an object beyond its range. Sensors on the satellite were used to track 14 sub-satellite targets released in two groups from the Delta. Otten said the problem developed when the optical sensor located an object, looked away, and then tried to relocate the original object. By then, the target had moved beyond the range of the sensor. The tracking test using the optical sensor was halted when the fuel budgeted for that exercise became exhausted, Otten said. He said an infrared sensor also failed to function properly, but infrared data was collected by a back-up system. Otten said only about three percent of the data collected during the 12-hour mission of Delta 181 has been examined.\",\n",
       " 'AP880212-0048': \"Infractions</HEAD> <NOTE>EDs: SUBS penultimate graf bgng ``Mica was not'' to CORRECT spelling of spokesman's last name to Gersuk, sted Gersick as sent.</NOTE> <BYLINE>By JOAN MOWER</BYLINE> <BYLINE>Associated Press Writer</BYLINE> The Marine Corps and the State Department on Friday minimized a report that more than 500 instances of possible wrongdoing by Marine guards at embassies worldwide had not been investigated properly. ``Many of the so-called `infractions' were very minor, such as curfew violations, insubordination, and misuse of telephone that could have no conceivable tie to suspected KGB activity,'' the corps said in a statement. The KGB is the Soviet spy agency. Both agencies responded to Rep. Dan Mica's statement that more than 500 reports of infractions by Marines at embassies had remained in the State Department's files until recently. Mica, D-Fla., said the reports, which dealt with black market activities or fraternization with Soviets, are supposed to be turned over to the Naval Investigative Service for further investigation. Mica said his information came from the General Accounting Office, the congressional watchdog agency, which began looking into security at the embassies in the wake of allegations of espionage at the Moscow embassy. The KGB often encourages people to engage in illegal behavior as a first step to gaining cooperation with the Soviets, Mica said. A State Department official, speaking on condition he remain anonymous, said the decision to turn over information about a possible violation by a Marine guard is sometimes made by the Marines, sometimes by the department. ``There have been instances where one or both passed'' along information to the NIS, the official said. The removal of a Marine is also a joint decision, he said. Asked about the 578 reports referred to by Mica, the corps said, ``The report is actually a listing of the Marine security guards whose tours of duty as embassy guards were cut short for a variety of reasons during the period 1980-1987.'' Some Marines were removed for disciplinary reasons; others for medical reasons or for giving a substandard performance, the statement said. Of the security guards sent home between 1985-1987 ``only a handful could be said to relate to espionage.'' During that period, only one Marine, Sgt. Clayton Lonetree, has been convicted of espionage for his activities in Moscow. The Marines said they had not known about the list of 578 until June 1987 ``when it surfaced at the Department of State.'' Mica was not available to comment because he is in Florida preparing to announce his candidacy for U.S. Senate. But his spokesman, John Gersuk, said infractions on the list could have been things other than black marketeering or fraternization violations. Mica has said the various agencies' procedures for handling potential security problems are chaotic and may need to be overhauled.\",\n",
       " 'AP880212-0049': \"<BYLINE>By SHARON L. JONES</BYLINE> <BYLINE>Associated Press Writer</BYLINE> The Hotel del Coronado, a lavish Victorian seaside resort with a star-studded past, will celebrate its centennial next weekend with an extravangaza befitting its most gilded days. Built in 1888 for $1 million, the hotel is one of the world's largest wooden structures and the nation's only working commercial hotel with a national monument designation. Famous for its Queen Anne design, the Del, as the hotel is known locally, has served as host to 10 presidents, royalty from around the world and countless celebrities. Businessman Montgomery Ward, actors Charlie Chaplin and Mary Pickford, and President Abraham Lincoln's son, Robert Todd, are among those who signed its registry. Thomas Edison was the five-story hotel's lighting technician. The picturesque resort of red-roofed turrets atop a white gingerbread exterior on a peninsula south of San Diego also has been a backdrop for a number of films and television shows. ``The Hotel Del is something fantastic,'' said silent screen star Anita Paige, who stayed at the hotel while filming ``The Flying Fleet'' in 1927. ``I've been to Europe since and their only competition is in France.'' A posh weekend party celebrating the 100th birthday of the wooden castle's Feb. 19, 1888, opening begins Friday night. Some 750 guests, including dozens of celebrities and movie stars, have paid $5,000 a couple to participate in the full weekend. Proceeds will be donated to charity. At Friday night's festivities for 2,000, a disc jockey resembling a hip Edison will spin records in one room as musicians and dancers perform in others. Marilyn Monroe look-alikes and original Munchkins from the movie ``The Wizard of Oz'' will wander the paneled hallways. The film ``Some Like it Hot,'' starring Miss Monroe, Jack Lemmon and Tony Curtis, was filmed on location at the hotel in 1958, and Oz author L. Frank Baum, a frequent hotel guest, allegedly based his book's Emerald City on the Del. Saturday will feature a harbor tour with America's Cup victor Dennis Conner, a celebrity tennis tournament and a ball, followed by a turn-of-the-century style picnic Sunday with hot air balloons and a Navy air show. Comedians George Burns and Phyllis Diller will be among the weekend's entertainers. But as current owner Larry Lawrence tells it, the hotel itself nearly didn't make it to the party. When he bought the 399-room hotel in 1963, it had fallen prey to termites and neglect. ``There was no electricity in many places, no water above the second floor,'' said Lawrence, the hotel's sixth owner. ``It was infested with termites and rats. This was a property that had everything but the bulldozer gutting it down.'' More than $40 million later, the hotel stands with its stateliness restored. It has new foundations, plumbing, heating, electricity and an $8 million fire protection system. A steel frame was implanted to bolster the Crown Room, a monumental dining room with a sugar pine ceiling that rises 33 feet. The hotel also was expanded. In 1970, the Crown Room was the site of the largest state dinner outside Washington, D.C., with then-President Richard Nixon, outgoing President Lyndon Johnson, Mexican President Gustavo Diaz Ordaz and 800 others. Today, about half of the hotel's guests are corporate executives. With 1,400 employees serving 700 suites, guests pay a premium for service, Lawrence said. The hotel's average rate is $200 a day. It all began as a dream of railroad tycoon Elisha Babcock and piano magnate H.L. Story, who envisioned while hunting on the barren peninsula a resort that would be ``the talk of the Western world.'' It was designed by three brothers, former railroad architects James, Merritt and Watson Reid, and built largely by unskilled Chinese laborers. It became the largest structure outside of New York City to be electrically lighted, powered by the first power plant in California.\",\n",
       " 'AP880212-0050': \"Firefighters recovered the bodies of two people Friday after a six-alarm blaze roared through an apartment complex, causing more than $1 million in damage and leaving 200 people homeless. Authorities have not determined the cause of the fire at Harvey's Racquet Apartments. All residents were accounted for. ``Firefighters found one adult and one child inside the building and were still searching,'' Fire Capt. Donna Cooper said. The victims were not immediately identified. Three others were slightly hurt, including one firefighter with a knee injury. The fire destroyed or damaged 48 apartments, officials said. About 120 firefighters battled the fast-moving fire at the apartments near Love Field in 20-degree weather. Witnesses said some residents escaped from second and third-floor apartments by climbing or jumping from balcony-to-balcony. In addition to the those left homeless, another 100 residents lost electricity because the power was cut for several hours. Complex employees and American Red Cross volunteers worked through the night to find temporary housing for residents. Most were temporarily housed in vacant apartments in other areas of the complex. Fire officials said it was the largest blaze in Dallas since flames swept through the Willow Creek Apartment complex five years ago.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(doc_texts.items())[43:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513c96dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens in the dictionary: 1112170\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set()\n",
    "\n",
    "# Function to add tokens to the set\n",
    "def add_tokens(value):\n",
    "    if isinstance(value, str):\n",
    "        # Split the string and add tokens to the set\n",
    "        unique_tokens.update(value.split())\n",
    "    elif isinstance(value, list):\n",
    "        # If the value is a list, iterate through each element\n",
    "        for item in value:\n",
    "            add_tokens(item)\n",
    "\n",
    "# Iterate through each key-value pair \n",
    "for value in doc_texts.values():\n",
    "    add_tokens(value)\n",
    "\n",
    "print(\"Total number of unique tokens in the dictionary:\", len(unique_tokens))\n",
    "#print(\"Unique tokens:\", unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db476912-1b42-406e-a72a-f887e97ab337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:33:47.167619Z",
     "start_time": "2024-06-10T09:33:46.296738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings\n",
    "  \n",
    "#word_embeddings_path = os.path.join(\"../\", \"NTLM_Experiment\", \"GoogleNews-vectors-negative300-SLIM.bin\")   # download the pretrained model\n",
    "word_embeddings_path = r\"C:\\Users\\dolla\\NTLM\\GoogleNews-vectors-negative300-SLIM.bin\" \n",
    "\n",
    "word_embeddings = gensim.models.KeyedVectors.load_word2vec_format(word_embeddings_path, binary=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facb189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(DirichletLM): 100%|██████████| 150/150 [00:11<00:00, 13.52q/s]\n",
      "Processing Topics: 100%|██████████| 150/150 [1:28:44<00:00, 35.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pyterrier as pt\n",
    "\n",
    "# Function to calculate term frequencies for documents and collection\n",
    "def calculate_term_frequencies(doc_texts):\n",
    "    term_frequencies = {}\n",
    "    collection_frequencies = defaultdict(int)\n",
    "    total_terms_in_collection = 0\n",
    "    \n",
    "    for doc_id, text in doc_texts.items():\n",
    "        term_freq = defaultdict(int)\n",
    "        document_terms = text.split()  # Split text into terms\n",
    "        total_terms_in_doc = len(document_terms)\n",
    "        \n",
    "        for term in document_terms:\n",
    "            term_freq[term] += 1\n",
    "            collection_frequencies[term] += 1\n",
    "            total_terms_in_collection += 1\n",
    "        \n",
    "        term_frequencies[doc_id] = (term_freq, total_terms_in_doc)\n",
    "    \n",
    "    return term_frequencies, collection_frequencies, total_terms_in_collection\n",
    "\n",
    "# Function to calculate the probability of term u given document d\n",
    "def p_u_given_d(term, doc_term_freq, total_terms_in_doc):\n",
    "    return doc_term_freq[term] / total_terms_in_doc if total_terms_in_doc > 0 else 0\n",
    "\n",
    "# Function to calculate the probability of term u in the collection C\n",
    "def p_u_given_C(term, collection_frequencies, total_terms_in_collection):\n",
    "    return collection_frequencies[term] / total_terms_in_collection if total_terms_in_collection > 0 else 0\n",
    "\n",
    "# Compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Compute translation probability using cosine similarity\n",
    "def compute_translation_probability(target_word, candidate_term, word_embeddings):\n",
    "    if target_word in word_embeddings and candidate_term in word_embeddings:\n",
    "        target_vector = word_embeddings[target_word]\n",
    "        candidate_vector = word_embeddings[candidate_term]\n",
    "        # Normalize cosine similarity to [0, 1]\n",
    "        return (cosine_similarity(target_vector, candidate_vector)+1)/2 \n",
    "    else:\n",
    "        return 0.0 \n",
    "\n",
    "# Compute log-likelihood ratio\n",
    "def compute_log_likelihood_ratio(translation_prob, p_qi_C, alpha, n):\n",
    "    return (np.log(translation_prob / (p_qi_C * alpha)) + n * np.log(alpha))\n",
    "\n",
    "# Score documents based on translation probabilities\n",
    "def score_document(query, document, word_embeddings, doc_term_freq, total_terms_in_doc, collection_frequencies, total_terms_in_collection, alpha=0.07):\n",
    "    score = 0.0\n",
    "    n = len(query)\n",
    "\n",
    "    for query_term in query:\n",
    "        if query_term not in word_embeddings:\n",
    "            continue  # Skip query terms not in word embeddings\n",
    "\n",
    "        translation_prob_sum = 0.0\n",
    "\n",
    "        for doc_term in document:\n",
    "            if doc_term not in word_embeddings:\n",
    "                continue  # Skip document terms not in word embeddings\n",
    "\n",
    "            p_qi_d = p_u_given_d(query_term, doc_term_freq, total_terms_in_doc) # ........Eq(4)\n",
    "            p_qi_C = p_u_given_C(query_term, collection_frequencies, total_terms_in_collection) # ........Eq(5)\n",
    "            translation_prob = compute_translation_probability(query_term, doc_term, word_embeddings)\n",
    "            translation_prob_sum += (translation_prob * p_qi_d)  # ......Eq(3)\n",
    "        \n",
    "        if translation_prob_sum > 0:\n",
    "            log_likelihood_ratio = compute_log_likelihood_ratio(translation_prob_sum, p_qi_C, alpha, n) # .....Eq(1)\n",
    "            score += log_likelihood_ratio\n",
    "\n",
    "    return score\n",
    "\n",
    "dirichlet = pt.BatchRetrieve(index_path, wmodel=\"DirichletLM\", controls={'dirichletlm.mu': 1000}, verbose=True) # , num_results=10\n",
    "\n",
    "# Retrieve and rank documents\n",
    "def retrieve_and_rank_documents(doc_texts, topics, word_embeddings, alpha=0.07):\n",
    "    results = []\n",
    "    \n",
    "    # Calculate term frequencies for documents and collection\n",
    "    term_frequencies, collection_frequencies, total_terms_in_collection = calculate_term_frequencies(doc_texts)\n",
    "    \n",
    "    # Retrieve initial set of documents using Dirichlet language model\n",
    "    result = dirichlet.transform(topics)\n",
    "    \n",
    "    for idx, row in tqdm(topics.iterrows(), total=len(topics), desc=\"Processing Topics\", position=0, leave=True):\n",
    "        topic_id = row['qid']\n",
    "        query = row['query'].split()\n",
    "        scores = []\n",
    "        \n",
    "        retrieved_docs = result.loc[result[\"qid\"] == topic_id]['docno'].values\n",
    "        \n",
    "        # Iterate over all document IDs in the topics\n",
    "        for doc_id in doc_texts.keys():\n",
    "            if doc_id not in retrieved_docs:\n",
    "                continue\n",
    "            doc_text = doc_texts[doc_id]\n",
    "            doc_terms = doc_text.split()\n",
    "            doc_term_freq, total_terms_in_doc = term_frequencies[doc_id]\n",
    "            score = score_document(query, doc_terms, word_embeddings, doc_term_freq, total_terms_in_doc, collection_frequencies, total_terms_in_collection, alpha)\n",
    "            scores.append((doc_id, score))\n",
    "        \n",
    "        ranked_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        for rank, (doc_id, score) in enumerate(ranked_scores):\n",
    "            results.append({\n",
    "                'qid': topic_id,\n",
    "                'docno': doc_id,\n",
    "                'rank': rank + 1,\n",
    "                'score': score,\n",
    "                'query': ' '.join(query)\n",
    "            })\n",
    "    return pd.DataFrame(results, columns=['qid', 'docno', 'rank', 'score', 'query'])\n",
    "\n",
    "retrieved_results = retrieve_and_rank_documents(doc_texts, topics, word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2c2f2a88abd4f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:53:27.915144Z",
     "start_time": "2024-06-10T09:53:24.207268Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BR(DirichletLM): 100%|██████████| 148/148 [00:04<00:00, 33.03q/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>P_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NTLM</td>\n",
       "      <td>0.086965</td>\n",
       "      <td>0.135135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dirichlet</td>\n",
       "      <td>0.184347</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name       map      P_10\n",
       "0       NTLM  0.086965  0.135135\n",
       "1  Dirichlet  0.184347  0.300000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt.Experiment([retrieved_results,dirichlet], topics, qrels, eval_metrics=[\"map\", \"P_10\"], names=[\"NTLM\", \"Dirichlet\"], filter_by_qrels=True,\n",
    "              filter_by_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809a8a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.25319642029633127\n"
     ]
    }
   ],
   "source": [
    "# Compute Mean Average Precision (MAP)\n",
    "def calculate_map(retrieved_results, qrels):\n",
    "    avg_precision = []\n",
    "    for qid in retrieved_results['qid'].unique():\n",
    "        relevant_docs = qrels[qrels['qid'] == qid]['docno'].tolist()\n",
    "        retrieved_docs = retrieved_results[retrieved_results['qid'] == qid]['docno'].tolist()\n",
    "        \n",
    "        precision_at_k = []\n",
    "        num_relevant = 0\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            if doc in relevant_docs:\n",
    "                num_relevant += 1\n",
    "                precision_at_k.append(num_relevant / (i + 1))\n",
    "        \n",
    "        if len(precision_at_k) > 0:\n",
    "            avg_precision.append(np.mean(precision_at_k))\n",
    "        else:\n",
    "            avg_precision.append(0.0)\n",
    "    \n",
    "    return np.mean(avg_precision)\n",
    "\n",
    "map_score = calculate_map(retrieved_results, qrels)\n",
    "print(f\"Mean Average Precision (MAP): {map_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81ee39",
   "metadata": {},
   "source": [
    "### Calculate MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf070df8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T09:25:58.449807Z",
     "start_time": "2024-06-10T09:25:50.277299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Score: 0.25560750142638755\n"
     ]
    }
   ],
   "source": [
    "def calculate_average_precision(relevance_labels):\n",
    "    precision_sum = 0.0\n",
    "    num_relevant_docs = 0\n",
    "    num_relevant_retrieved = 0\n",
    "    \n",
    "    for i, label in enumerate(relevance_labels):\n",
    "        if label == 1:\n",
    "            num_relevant_retrieved += 1\n",
    "            precision = num_relevant_retrieved / (i + 1)\n",
    "            precision_sum += precision\n",
    "            num_relevant_docs += 1\n",
    "    \n",
    "    if num_relevant_docs == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return precision_sum / num_relevant_docs\n",
    "\n",
    "def calculate_map(retrieved_results, qrels):\n",
    "    grouped_results = retrieved_results.groupby('qid')\n",
    "    total_average_precision = 0.0\n",
    "    num_queries = 0\n",
    "    \n",
    "    for query_id, group in grouped_results:\n",
    "        retrieved_docnos = group['docno'].values\n",
    "        \n",
    "        relevance_labels = [1 if (docno in qrels[(qrels['qid'] == query_id) & (qrels['docno'] == docno)]['docno'].values) else 0 for docno in retrieved_docnos]\n",
    "        \n",
    "        #print(f\"Query ID: {query_id}\")\n",
    "        #print(f\"Retrieved Docnos: {retrieved_docnos}\")\n",
    "        #print(f\"Relevance Labels: {relevance_labels}\")\n",
    "        \n",
    "        average_precision = calculate_average_precision(relevance_labels)\n",
    "        #print(f\"Average Precision: {average_precision}\")\n",
    "        \n",
    "        total_average_precision += average_precision\n",
    "        num_queries += 1\n",
    "    \n",
    "    map_score = total_average_precision / num_queries\n",
    "    return map_score\n",
    "\n",
    "# Calculate MAP\n",
    "map_score = calculate_map(retrieved_results, qrels)\n",
    "print(\"MAP Score:\", map_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db21a10",
   "metadata": {},
   "source": [
    "### Calculate P@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6156aa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P@10 Score: 0.3413333333333334\n"
     ]
    }
   ],
   "source": [
    "def calculate_precision_at_k(relevance_labels, k):\n",
    "   \n",
    "   \n",
    "    num_relevant_retrieved = sum(relevance_labels[:k])\n",
    "    return num_relevant_retrieved / k\n",
    "\n",
    "def calculate_p_at_10(retrieved_results, qrels):\n",
    "  \n",
    "  \n",
    "    grouped_results = retrieved_results.groupby('qid')\n",
    "    total_precision_at_10 = 0.0\n",
    "    num_queries = 0\n",
    "    \n",
    "    for query_id, group in grouped_results:\n",
    "        retrieved_docnos = group['docno'].values\n",
    "        \n",
    "        relevance_labels = [1 if (docno in qrels[(qrels['qid'] == query_id) & (qrels['docno'] == docno)]['docno'].values) else 0 for docno in retrieved_docnos]\n",
    "        \n",
    "        precision_at_10 = calculate_precision_at_k(relevance_labels, 10)\n",
    "        \n",
    "        total_precision_at_10 += precision_at_10\n",
    "        num_queries += 1\n",
    "    \n",
    "    average_precision_at_10 = total_precision_at_10 / num_queries\n",
    "    return average_precision_at_10\n",
    "\n",
    "# Example usage:\n",
    "p_at_10_score = calculate_p_at_10(retrieved_results, qrels)\n",
    "print(\"P@10 Score:\", p_at_10_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "200494db8dd1e96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at 10 (P@10): 0.3413333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_p_at_k(retrieved_results, qrels, k=10):\n",
    "    precision_scores = []\n",
    "    for qid in retrieved_results['qid'].unique():\n",
    "        relevant_docs = set(qrels[qrels['qid'] == qid]['docno'].tolist())\n",
    "        retrieved_docs = retrieved_results[retrieved_results['qid'] == qid]['docno'].tolist()[:k]\n",
    "        \n",
    "        num_relevant = len(set(retrieved_docs).intersection(relevant_docs))\n",
    "        precision_at_k = num_relevant / k if k > 0 else 0.0\n",
    "        \n",
    "        precision_scores.append(precision_at_k)\n",
    "    \n",
    "    return np.mean(precision_scores)\n",
    "\n",
    "# Example usage:\n",
    "p_at_10_score = calculate_p_at_k(retrieved_results, qrels, k=10)\n",
    "print(f\"Precision at 10 (P@10): {p_at_10_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the embeddings from the .bin file\n",
    "def load_embeddings(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        embeddings = np.load(f, allow_pickle=True).item()\n",
    "    return embeddings\n",
    "\n",
    "# Function to calculate term frequencies for documents and collection\n",
    "def calculate_term_frequencies(doc_texts):\n",
    "    term_frequencies = {}\n",
    "    collection_frequencies = defaultdict(int)\n",
    "    total_terms_in_collection = 0\n",
    "    \n",
    "    for doc_id, text in doc_texts.items():\n",
    "        term_freq = defaultdict(int)\n",
    "        document_terms = text.split()  # Split text into terms\n",
    "        total_terms_in_doc = len(document_terms)\n",
    "        \n",
    "        for term in document_terms:\n",
    "            term_freq[term] += 1\n",
    "            collection_frequencies[term] += 1\n",
    "            total_terms_in_collection += 1\n",
    "        \n",
    "        term_frequencies[doc_id] = (term_freq, total_terms_in_doc)\n",
    "    \n",
    "    return term_frequencies, collection_frequencies, total_terms_in_collection\n",
    "\n",
    "# Compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Compute translation probability using cosine similarity with BERT embeddings\n",
    "def compute_translation_probability(target_word, candidate_term, word_embeddings):\n",
    "    if target_word in word_embeddings and candidate_term in word_embeddings:\n",
    "        target_vector = word_embeddings[target_word]\n",
    "        candidate_vector = word_embeddings[candidate_term]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(target_vector, candidate_vector)\n",
    "        \n",
    "        # Normalize cosine similarity to [0, 1]\n",
    "        return (similarity + 1) / 2\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate the probability of term u given document d\n",
    "def p_u_given_d(term, doc_term_freq, total_terms_in_doc):\n",
    "    return doc_term_freq[term] / total_terms_in_doc if total_terms_in_doc > 0 else 0\n",
    "\n",
    "# Function to calculate the probability of term u in the collection C\n",
    "def p_u_given_C(term, collection_frequencies, total_terms_in_collection):\n",
    "    return collection_frequencies[term] / total_terms_in_collection if total_terms_in_collection > 0 else 0\n",
    "\n",
    "# Compute log-likelihood ratio\n",
    "def compute_log_likelihood_ratio(translation_prob, p_qi_C, alpha, n):\n",
    "    return (np.log(translation_prob / (p_qi_C * alpha)) + n * np.log(alpha))\n",
    "\n",
    "# Score documents based on translation probabilities using BERT embeddings\n",
    "def score_document(query, document, word_embeddings, doc_term_freq, total_terms_in_doc, collection_frequencies, total_terms_in_collection, alpha=0.1):\n",
    "    score = 0.0\n",
    "    n = len(query)\n",
    "\n",
    "    for query_term in query:\n",
    "        if query_term not in word_embeddings:\n",
    "            continue  # Skip query terms not in word embeddings\n",
    "\n",
    "        translation_prob_sum = 0.0\n",
    "\n",
    "        for doc_term in document:\n",
    "            if doc_term not in word_embeddings:\n",
    "                continue  # Skip document terms not in word embeddings\n",
    "\n",
    "            p_qi_d = p_u_given_d(query_term, doc_term_freq, total_terms_in_doc) # Eq(4)\n",
    "            p_qi_C = p_u_given_C(query_term, collection_frequencies, total_terms_in_collection) # Eq(5)\n",
    "            translation_prob = compute_translation_probability(query_term, doc_term, word_embeddings)\n",
    "            translation_prob_sum += (translation_prob * p_qi_d)  # Eq(3)\n",
    "        \n",
    "        if translation_prob_sum > 0:\n",
    "            log_likelihood_ratio = compute_log_likelihood_ratio(translation_prob_sum, p_qi_C, alpha, n) # Eq(1)\n",
    "            score += log_likelihood_ratio\n",
    "\n",
    "    return score\n",
    "\n",
    "# Example usage with Terrier\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "dirichlet = pt.BatchRetrieve(\"index_path\", wmodel=\"DirichletLM\", controls={'dirichletlm.mu': 1000}, verbose=True)\n",
    "\n",
    "# Retrieve and rank documents using BERT embeddings\n",
    "def retrieve_and_rank_documents(doc_texts, topics, word_embeddings, alpha=0.1):\n",
    "    results = []\n",
    "    \n",
    "    # Calculate term frequencies for documents and collection\n",
    "    term_frequencies, collection_frequencies, total_terms_in_collection = calculate_term_frequencies(doc_texts)\n",
    "    \n",
    "    # Retrieve initial set of documents using Dirichlet language model\n",
    "    result = dirichlet.transform(topics)\n",
    "    \n",
    "    for idx, row in tqdm(topics.iterrows(), total=len(topics), desc=\"Processing Topics\", position=0, leave=True):\n",
    "        topic_id = row['qid']\n",
    "        query = row['query'].split()\n",
    "        scores = []\n",
    "        \n",
    "        retrieved_docs = result.loc[result[\"qid\"] == topic_id]['docno'].values\n",
    "        \n",
    "        # Iterate over all document IDs in the topics\n",
    "        for doc_id in doc_texts.keys():\n",
    "            if doc_id not in retrieved_docs:\n",
    "                continue\n",
    "            doc_text = doc_texts[doc_id]\n",
    "            doc_terms = doc_text.split()\n",
    "            doc_term_freq, total_terms_in_doc = term_frequencies[doc_id]\n",
    "            score = score_document(query, doc_terms, word_embeddings, doc_term_freq, total_terms_in_doc, collection_frequencies, total_terms_in_collection, alpha)\n",
    "            scores.append((doc_id, score))\n",
    "        \n",
    "        ranked_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        for rank, (doc_id, score) in enumerate(ranked_scores):\n",
    "            results.append({\n",
    "                'qid': topic_id,\n",
    "                'docno': doc_id,\n",
    "                'rank': rank + 1,\n",
    "                'score': score,\n",
    "                'query': ' '.join(query)\n",
    "            })\n",
    "    return pd.DataFrame(results, columns=['qid', 'docno', 'rank', 'score', 'query'])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the BERT embeddings .bin file\n",
    "    embeddings_file_path = 'path_to_your_embeddings_file.bin'\n",
    "    \n",
    "    # Load word embeddings\n",
    "    word_embeddings = load_embeddings(embeddings_file_path)\n",
    "    \n",
    "    # Assuming 'doc_texts' is a dictionary of document IDs and their corresponding text\n",
    "    # Assuming 'topics' is a DataFrame with columns 'qid' and 'query'\n",
    "    retrieved_results = retrieve_and_rank_documents(doc_texts, topics, word_embeddings)\n",
    "\n",
    "    # Save or process retrieved_results as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
